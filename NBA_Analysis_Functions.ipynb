{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0afdcb6-9345-49a5-afd2-df444ffc6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate and display a correlation matrix for the full dataset\n",
    "def gen_corr_matrix(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Generate a correlation matrix for the given DataFrame and return a new DataFrame containing \n",
    "    features with strong correlations.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame for which to compute the correlation matrix.\n",
    "    threshold (float): The threshold above which correlations are considered strong.\n",
    "        \n",
    "    Returns:\n",
    "    correlation_matrix (pd.DataFrame): The correlation matrix of the data.\n",
    "    strong_correlations (pd.DataFrame): DataFrame containing features with strong correlations.\n",
    "    \"\"\"\n",
    "    # import the pandas library\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    # Unstack the correlation matrix to create a Series of correlation pairs\n",
    "    correlation_pairs = correlation_matrix.unstack()\n",
    "    \n",
    "    # Convert the Series to a DataFrame and reset the index\n",
    "    correlation_pairs_df = pd.DataFrame(correlation_pairs, columns=['Correlation']).reset_index()\n",
    "    correlation_pairs_df.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "    \n",
    "    # Filter the pairs to show only those with strong correlations\n",
    "    strong_correlations = correlation_pairs_df[\n",
    "        (correlation_pairs_df['Correlation'].abs() > threshold) & \n",
    "        (correlation_pairs_df['Feature1'] != correlation_pairs_df['Feature2'])\n",
    "    ]\n",
    "    \n",
    "    # Remove duplicate pairs (e.g., (A, B) and (B, A))\n",
    "    strong_correlations['Pair'] = strong_correlations.apply(\n",
    "        lambda row: tuple(sorted([row['Feature1'], row['Feature2']])), axis=1\n",
    "    )\n",
    "    strong_correlations = strong_correlations.drop_duplicates(subset=['Pair']).drop(columns=['Pair'])\n",
    "    \n",
    "    return correlation_matrix, strong_correlations\n",
    "\n",
    "# Create a function to display the correlation matrix\n",
    "def show_matrix(matrix, target):\n",
    "    \"\"\"\n",
    "    Function to display the correlation matrix.\n",
    "\n",
    "    Parameters:\n",
    "    matrix (pd.DataFrame): DataFrame containing feature correlations.\n",
    "    target (string): Target feature.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the matplotlib.pyplot and seaborn libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Visualize the correlation matrix for the dataset minus the target feature and 'win_percentage' using a heatmap\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title(f'{target} Correlations')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "# Create a function to drop redundant features from the dataset to mitigate high multicollinearity within the model\n",
    "def drop_redundant_feats(df, df1):\n",
    "    \"\"\"\n",
    "    A function to drop features from a dataset that will cause high multicollinearity in the Lasso regression model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing features with high correlations.\n",
    "    df1 (pd.DataFrame): DataFrame containing features from dataset.\n",
    "\n",
    "    Returns:\n",
    "    df_reduced_feats (pd.DataFrame): DataFrame with data from selected features.\n",
    "    \"\"\"\n",
    "    # Create a list of redundant features to drop\n",
    "    redundant_feats = df['Feature1'].unique()\n",
    "\n",
    "    # Create a copy of df1 to avoid modifying the original DataFrame\n",
    "    df_reduced_feats = df1.copy()\n",
    "    \n",
    "    # Drop the redundant features from the copied DataFrame\n",
    "    df_reduced_feats.drop(columns=redundant_feats, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_reduced_feats\n",
    "\n",
    "# Create a function to determine variance inflation factors\n",
    "def gen_vif(X):\n",
    "    \"\"\"\n",
    "    Function to determine variance inflation factors.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): DataFrame containing the feature set.\n",
    "\n",
    "    Returns:\n",
    "    vif_data (pd.DataFrame): DataFrame containing features and their variance inflation factor scores.\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import pandas as pd\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    # Create a copy of X to preserve the original data\n",
    "    X_copy = X.copy()\n",
    "    X_copy['intercept'] = 1\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X_copy.columns\n",
    "    vif_data['vif'] = [variance_inflation_factor(X_copy.values, i) for i in range(X_copy.shape[1])]\n",
    "\n",
    "    return vif_data\n",
    "\n",
    "# Create a function to fit and train a lasso model, returning a dictionary of various metrics and an updated feature DataFrame\n",
    "def gen_lasso_model(feats, y, alpha_grid=None, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Lasso Regression model after finding the best alpha. Returns a dictionary containing the model, performance \n",
    "    metrics, non-zero coefficients, feature names, y measurables, and an updated DataFrame with feature data. \n",
    "    \n",
    "    Parameters:\n",
    "    feats (pd.DataFrame): DataFrame containing feature variables.\n",
    "    y (pd.Series): Target variable.\n",
    "    alpha_grid (list or None): List of alpha values to search. If None, default grid is used.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    results (Dictionary): Dictionary containing the model, performance metrics, non-zero coefficients, and feature names.\n",
    "    updated_df (pd.DataFrame): DataFrame containing updated feature data.\n",
    "    \"\"\"\n",
    "    # Import needed libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Create an alpha grid\n",
    "    if alpha_grid is None:\n",
    "        alpha_grid = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "    y_measurables = y.describe()\n",
    "\n",
    "    while True:\n",
    "        # Store the feature names\n",
    "        feature_names = feats.columns\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(feats)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Create the grid search object\n",
    "        lasso_grid = GridSearchCV(Lasso(), {'alpha': alpha_grid}, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        # Fit the grid to the data\n",
    "        lasso_grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the best alpha value\n",
    "        best_alpha = lasso_grid.best_params_['alpha']\n",
    "        \n",
    "        # Retrain the new model using the best alpha score\n",
    "        lasso = Lasso(alpha=best_alpha)\n",
    "        lasso.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_train_pred = lasso.predict(X_train)\n",
    "        y_test_pred = lasso.predict(X_test)\n",
    "\n",
    "        # Performance metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Get coefficients from new model\n",
    "        coefficients = pd.Series(lasso.coef_, index=feature_names)\n",
    "\n",
    "        # Calculate and store non-zero coefficients\n",
    "        non_zero_coefficients = coefficients[coefficients != 0].sort_values(ascending=False)\n",
    "\n",
    "        # Calculate and store features with coefficients of zero\n",
    "        zero_coefficients = coefficients[coefficients == 0].index.tolist()\n",
    "\n",
    "        # Create an updated dataset \n",
    "        updated_df = feats.drop(zero_coefficients, axis=1)\n",
    "\n",
    "        # If there are no zero coefficients, break the loop\n",
    "        if len(zero_coefficients) == 0:\n",
    "            break\n",
    "        \n",
    "        # Update feats for the next iteration\n",
    "        feats = updated_df\n",
    "\n",
    "    # Call the gen_vif function to calculate variance inflation factors\n",
    "    vif_data = gen_vif(pd.DataFrame(X_scaled, columns=feature_names))\n",
    "\n",
    "    # Print the best alpha, y measurables, metric scores, non-zero coefficients, and vif data\n",
    "    print(f'Best Alpha: {best_alpha}')\n",
    "    print(f'Y Measurables: \\n{y_measurables}')\n",
    "    print(f'Training RMSE: {train_rmse}, Testing RMSE: {test_rmse}')\n",
    "    print(f'Training R^2: {train_r2}, Testing R^2: {test_r2}')\n",
    "    print(f'Non-zero Coefficients: \\n{non_zero_coefficients}')\n",
    "    print(f'Variance Inflation Factors: \\n{vif_data}')\n",
    "\n",
    "    # Return the results as a dictionary\n",
    "    results = {\n",
    "        'model': lasso,\n",
    "        'X': X_scaled,\n",
    "        'y': y,\n",
    "        'non_zero_coefficients': non_zero_coefficients,\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "    return results, updated_df\n",
    "\n",
    "# Create a function to generate a horizontal bar chart displaying the regression coefficients to the target feature\n",
    "def gen_coeff_barh(coefficients, target):\n",
    "    \"\"\"\n",
    "    Function to generate a horizontal bar chart displaying feature regression coefficients to the target feature.\n",
    "\n",
    "    Parameters:\n",
    "    coefficients (pd.Series): Series of features and regression coefficient values.\n",
    "    target (string): Target feature.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Set the height proportionally with the length of coefficients\n",
    "    height = len(coefficients)\n",
    "    \n",
    "    # Visualize coefficients with a horizontal bar chart\n",
    "    plt.figure(figsize=(12, height))\n",
    "    coefficients.sort_values().plot(kind='barh')\n",
    "    plt.title(f'Coefficients for {target}')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "# Create a function to generate bootstrapped regression coefficients and calculate the 95% confidence intervals along with evaluation metrics\n",
    "def gen_bootstrap_coefficients(model, X, y, feature_names, n_bootstraps=1000):\n",
    "    \"\"\"\n",
    "    Generate bootstrapped regression coefficients and calculate 95% confidence intervals for a Lasso regression model. Returns and prints a DataFrame \n",
    "    with the sorted coefficient summary. Also prints average target value and evaluation metrics for the bootstrapped model. \n",
    "    \n",
    "    Parameters:\n",
    "    model (Lasso): A trained Lasso regression model.\n",
    "    X (DataFrame): Features data.\n",
    "    y_train (Series): Target variable.\n",
    "    feature_names (list): List of feature names.\n",
    "    n_bootstraps (int): Number of bootstrap samples.\n",
    "    \n",
    "    Returns:\n",
    "    sf_df (DataFrame): DataFrame containing the sorted results.\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.utils import resample\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    # Lists to store bootstrap coefficients, y means, RMSE, and R-squared scores\n",
    "    bootstrap_coefficients_list = []\n",
    "    train_rmse_scores = []\n",
    "    test_rmse_scores = []\n",
    "    train_r2_scores = []\n",
    "    test_r2_scores = []\n",
    "\n",
    "    # Bootstrap loop\n",
    "    for _ in range(n_bootstraps):\n",
    "        X_resampled, y_resampled = resample(X,y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Collect each series of coefficients\n",
    "        bootstrap_coefficients_list.append(pd.Series(model.coef_, index=feature_names))\n",
    "\n",
    "        # Predict and calculate RMSE and R^2 scores for the training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        train_rmse_scores.append(train_rmse)\n",
    "        train_r2_scores.append(train_r2)\n",
    "\n",
    "        # Predict and calculate RMSE and R^2 scores for the testing set\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        test_rmse_scores.append(test_rmse)\n",
    "        test_r2_scores.append(test_r2)\n",
    "        \n",
    "    # Concatenate all series stored in the list into a dataframe\n",
    "    bootstrap_coefficients = pd.concat(bootstrap_coefficients_list, axis=1).transpose()\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the bootstrapped coefficients\n",
    "    coefficient_mean = bootstrap_coefficients.mean()\n",
    "    coefficient_std = bootstrap_coefficients.std()\n",
    "    \n",
    "    # Calculate the 95% confidence intervals for each coefficient\n",
    "    ci_lower = bootstrap_coefficients.quantile(0.025)\n",
    "    ci_upper = bootstrap_coefficients.quantile(0.975)\n",
    "    \n",
    "    # Create a DataFrame to summarize the results\n",
    "    coefficient_summary = pd.DataFrame({\n",
    "        'Mean': coefficient_mean,\n",
    "        'StdDev': coefficient_std,\n",
    "        'CI Lower': ci_lower,\n",
    "        'CI Upper': ci_upper\n",
    "    })\n",
    "    \n",
    "    # Select significant features based on the absolute value of the coefficient mean and distribution\n",
    "    selected_features = coefficient_summary.index.tolist()\n",
    "\n",
    "    # Create a DataFrame from the bootstrap results and sort it by the mean coefficient\n",
    "    sf_df = coefficient_summary.loc[selected_features].sort_values(by='Mean', ascending=False)\n",
    "\n",
    "    # Calculate and store the average of the target variable\n",
    "    avg_y = np.mean(y)\n",
    "\n",
    "    # Calculate and store the mean RMSE and R^2 scores for both training and testing sets\n",
    "    avg_train_rmse = np.mean(train_rmse_scores)\n",
    "    avg_test_rmse = np.mean(test_rmse_scores)\n",
    "    avg_train_r2 = np.mean(train_r2_scores)\n",
    "    avg_test_r2 = np.mean(test_r2_scores)\n",
    "\n",
    "    # Print the evaluation metrics and the sorted coefficients\n",
    "    print(f'Average Y: {avg_y}')\n",
    "    print(f'Average Training RMSE: {avg_train_rmse}, Average Testing RMSE: {avg_test_rmse}')\n",
    "    print(f'Average Training R^2: {avg_train_r2}, Average Testing R^2: {avg_test_r2}')\n",
    "    print(f'Sorted Coefficients: \\n{sf_df}')\n",
    "    \n",
    "    return sf_df\n",
    "\n",
    "# Create a function to generate an error bar chart containing regression coefficients with 95% confidence intervals to the target feature\n",
    "def gen_ebar(bootstrap_results):\n",
    "    \"\"\"\n",
    "    Function to generate and display an error bar chart containing feature regression coefficients with 95% confidence intervals to the target feature.\n",
    "\n",
    "    Parameters:\n",
    "    bootstrap_results (pd.DataFrame): DataFrame containing the mean, standard deviation, and lower/upper confidence intervals of the regression \n",
    "    coefficients to the target feature.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Set the width proportionally with the length of coefficients\n",
    "    width = len(bootstrap_results)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(width, 6))\n",
    "    plt.errorbar(\n",
    "        bootstrap_results.index,\n",
    "        bootstrap_results['Mean'],\n",
    "        yerr=[\n",
    "            bootstrap_results['Mean'] - bootstrap_results['CI Lower'],\n",
    "            bootstrap_results['CI Upper'] - bootstrap_results['Mean']\n",
    "        ],\n",
    "        fmt='o',\n",
    "        capsize=5\n",
    "    )\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "    plt.title('Regression Coefficients with 95% Confidence Intervals')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "# Create a function to calculate the effects of the selected features on the target\n",
    "def effect_on_target(bootstrap_results, df, is_percentage=True):\n",
    "    \"\"\"\n",
    "    Calculate the effect on the target for each feature after bootstrapping occurs and print the results.\n",
    "\n",
    "    Parameters:\n",
    "    target (string): Name of target variable.\n",
    "    bootstrap_results (pd.DataFrame): DataFrame containing feature statistics with columns ['Mean', 'StdDev', 'CI Lower', 'CI Upper'].\n",
    "    df (pd.DataFrame): Original DataFrame containing features.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import numpy as np\n",
    "    \n",
    "    for index, row in bootstrap_results.iterrows():\n",
    "        ci_lower = row['CI Lower']\n",
    "        ci_upper = row['CI Upper']\n",
    "\n",
    "        # Exclude features whose confidence intervals include 0\n",
    "        if ci_lower <= 0 <= ci_upper:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        feature_name = index\n",
    "        regression_coefficient = row['Mean']\n",
    "\n",
    "        # Calculate the increment as 2% of the mean value of the feature\n",
    "        feature_mean = np.mean(df[feature_name])\n",
    "        increment = feature_mean * 0.02\n",
    "\n",
    "        # Get the standard deviation of the original (non-scaled) feature\n",
    "        feature_std = np.std(df[feature_name])\n",
    "\n",
    "        # Calculate the scaled feature increment\n",
    "        delta_x_scaled = increment / feature_std\n",
    "\n",
    "        # Calculate the effect on the target variable\n",
    "        delta_y = regression_coefficient * delta_x_scaled\n",
    "\n",
    "        # Print the effect statement\n",
    "        if is_percentage:\n",
    "            print(f'A {increment:.3f} unit increase in {feature_name} results in an approximate {delta_y:.4f} (or {delta_y * 100:.2f} percentage points) change in the target variable.')\n",
    "        else:\n",
    "            print(f\"A {increment:.3f} unit increase in {feature_name} results in an approximate {delta_y:.3f} change in the target variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbd4b23-93bd-47d4-9631-172bc3fc56f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nba_analysis_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nba_analysis_functions.py\n",
    "\n",
    "# Create a function to generate and display a correlation matrix for the full dataset\n",
    "def gen_corr_matrix(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Generate a correlation matrix for the given DataFrame and return a new DataFrame containing \n",
    "    features with strong correlations.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame for which to compute the correlation matrix.\n",
    "    threshold (float): The threshold above which correlations are considered strong.\n",
    "        \n",
    "    Returns:\n",
    "    correlation_matrix (pd.DataFrame): The correlation matrix of the data.\n",
    "    strong_correlations (pd.DataFrame): DataFrame containing features with strong correlations.\n",
    "    \"\"\"\n",
    "    # import the pandas library\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    # Unstack the correlation matrix to create a Series of correlation pairs\n",
    "    correlation_pairs = correlation_matrix.unstack()\n",
    "    \n",
    "    # Convert the Series to a DataFrame and reset the index\n",
    "    correlation_pairs_df = pd.DataFrame(correlation_pairs, columns=['Correlation']).reset_index()\n",
    "    correlation_pairs_df.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "    \n",
    "    # Filter the pairs to show only those with strong correlations\n",
    "    strong_correlations = correlation_pairs_df[\n",
    "        (correlation_pairs_df['Correlation'].abs() > threshold) & \n",
    "        (correlation_pairs_df['Feature1'] != correlation_pairs_df['Feature2'])\n",
    "    ]\n",
    "    \n",
    "    # Remove duplicate pairs (e.g., (A, B) and (B, A))\n",
    "    strong_correlations['Pair'] = strong_correlations.apply(\n",
    "        lambda row: tuple(sorted([row['Feature1'], row['Feature2']])), axis=1\n",
    "    )\n",
    "    strong_correlations = strong_correlations.drop_duplicates(subset=['Pair']).drop(columns=['Pair'])\n",
    "    \n",
    "    return correlation_matrix, strong_correlations\n",
    "\n",
    "# Create a function to display the correlation matrix\n",
    "def show_matrix(matrix, target):\n",
    "    \"\"\"\n",
    "    Function to display the correlation matrix.\n",
    "\n",
    "    Parameters:\n",
    "    matrix (pd.DataFrame): DataFrame containing feature correlations.\n",
    "    target (string): Target feature.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the matplotlib.pyplot and seaborn libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Visualize the correlation matrix for the dataset minus the target feature and 'win_percentage' using a heatmap\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title(f'{target} Correlations')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "# Create a function to drop redundant features from the dataset to mitigate high multicollinearity within the model\n",
    "def drop_redundant_feats(df, df1):\n",
    "    \"\"\"\n",
    "    A function to drop features from a dataset that will cause high multicollinearity in the Lasso regression model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing features with high correlations.\n",
    "    df1 (pd.DataFrame): DataFrame containing features from dataset.\n",
    "\n",
    "    Returns:\n",
    "    df_reduced_feats (pd.DataFrame): DataFrame with data from selected features.\n",
    "    \"\"\"\n",
    "    # Create a list of redundant features to drop\n",
    "    redundant_feats = df['Feature1'].unique()\n",
    "\n",
    "    # Create a copy of df1 to avoid modifying the original DataFrame\n",
    "    df_reduced_feats = df1.copy()\n",
    "    \n",
    "    # Drop the redundant features from the copied DataFrame\n",
    "    df_reduced_feats.drop(columns=redundant_feats, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_reduced_feats\n",
    "\n",
    "# Create a function to determine variance inflation factors\n",
    "def gen_vif(X):\n",
    "    \"\"\"\n",
    "    Function to determine variance inflation factors.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): DataFrame containing the feature set.\n",
    "\n",
    "    Returns:\n",
    "    vif_data (pd.DataFrame): DataFrame containing features and their variance inflation factor scores.\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import pandas as pd\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    # Create a copy of X to preserve the original data\n",
    "    X_copy = X.copy()\n",
    "    X_copy['intercept'] = 1\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X_copy.columns\n",
    "    vif_data['vif'] = [variance_inflation_factor(X_copy.values, i) for i in range(X_copy.shape[1])]\n",
    "\n",
    "    return vif_data\n",
    "\n",
    "# Create a function to fit and train a lasso model, returning a dictionary of various metrics and an updated feature DataFrame\n",
    "def gen_lasso_model(feats, y, alpha_grid=None, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Lasso Regression model after finding the best alpha. Returns a dictionary containing the model, performance \n",
    "    metrics, non-zero coefficients, feature names, y measurables, and an updated DataFrame with feature data. \n",
    "    \n",
    "    Parameters:\n",
    "    feats (pd.DataFrame): DataFrame containing feature variables.\n",
    "    y (pd.Series): Target variable.\n",
    "    alpha_grid (list or None): List of alpha values to search. If None, default grid is used.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    results (Dictionary): Dictionary containing the model, performance metrics, non-zero coefficients, and feature names.\n",
    "    updated_df (pd.DataFrame): DataFrame containing updated feature data.\n",
    "    \"\"\"\n",
    "    # Import needed libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Create an alpha grid\n",
    "    if alpha_grid is None:\n",
    "        alpha_grid = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "    y_measurables = y.describe()\n",
    "\n",
    "    while True:\n",
    "        # Store the feature names\n",
    "        feature_names = feats.columns\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(feats)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Create the grid search object\n",
    "        lasso_grid = GridSearchCV(Lasso(), {'alpha': alpha_grid}, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        # Fit the grid to the data\n",
    "        lasso_grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the best alpha value\n",
    "        best_alpha = lasso_grid.best_params_['alpha']\n",
    "        \n",
    "        # Retrain the new model using the best alpha score\n",
    "        lasso = Lasso(alpha=best_alpha)\n",
    "        lasso.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_train_pred = lasso.predict(X_train)\n",
    "        y_test_pred = lasso.predict(X_test)\n",
    "\n",
    "        # Performance metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Get coefficients from new model\n",
    "        coefficients = pd.Series(lasso.coef_, index=feature_names)\n",
    "\n",
    "        # Calculate and store non-zero coefficients\n",
    "        non_zero_coefficients = coefficients[coefficients != 0].sort_values(ascending=False)\n",
    "\n",
    "        # Calculate and store features with coefficients of zero\n",
    "        zero_coefficients = coefficients[coefficients == 0].index.tolist()\n",
    "\n",
    "        # Create an updated dataset \n",
    "        updated_df = feats.drop(zero_coefficients, axis=1)\n",
    "\n",
    "        # If there are no zero coefficients, break the loop\n",
    "        if len(zero_coefficients) == 0:\n",
    "            break\n",
    "        \n",
    "        # Update feats for the next iteration\n",
    "        feats = updated_df\n",
    "\n",
    "    # Call the gen_vif function to calculate variance inflation factors\n",
    "    vif_data = gen_vif(pd.DataFrame(X_scaled, columns=feature_names))\n",
    "\n",
    "    # Print the best alpha, y measurables, metric scores, non-zero coefficients, and vif data\n",
    "    print(f'Best Alpha: {best_alpha}')\n",
    "    print(f'Y Measurables: \\n{y_measurables}')\n",
    "    print(f'Training RMSE: {train_rmse}, Testing RMSE: {test_rmse}')\n",
    "    print(f'Training R^2: {train_r2}, Testing R^2: {test_r2}')\n",
    "    print(f'Non-zero Coefficients: \\n{non_zero_coefficients}')\n",
    "    print(f'Variance Inflation Factors: \\n{vif_data}')\n",
    "\n",
    "    # Return the results as a dictionary\n",
    "    results = {\n",
    "        'model': lasso,\n",
    "        'X': X_scaled,\n",
    "        'y': y,\n",
    "        'non_zero_coefficients': non_zero_coefficients,\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "    return results, updated_df\n",
    "\n",
    "# Create a function to generate a horizontal bar chart displaying the regression coefficients to the target feature\n",
    "def gen_coeff_barh(coefficients, target):\n",
    "    \"\"\"\n",
    "    Function to generate a horizontal bar chart displaying feature regression coefficients to the target feature.\n",
    "\n",
    "    Parameters:\n",
    "    coefficients (pd.Series): Series of features and regression coefficient values.\n",
    "    target (string): Target feature.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Set the height proportionally with the length of coefficients\n",
    "    height = len(coefficients)\n",
    "    \n",
    "    # Visualize coefficients with a horizontal bar chart\n",
    "    plt.figure(figsize=(12, height))\n",
    "    coefficients.sort_values().plot(kind='barh')\n",
    "    plt.title(f'Coefficients for {target}')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "# Create a function to generate bootstrapped regression coefficients and calculate the 95% confidence intervals along with evaluation metrics\n",
    "def gen_bootstrap_coefficients(model, X, y, feature_names, n_bootstraps=1000):\n",
    "    \"\"\"\n",
    "    Generate bootstrapped regression coefficients and calculate 95% confidence intervals for a Lasso regression model. Returns and prints a DataFrame \n",
    "    with the sorted coefficient summary. Also prints average target value and evaluation metrics for the bootstrapped model. \n",
    "    \n",
    "    Parameters:\n",
    "    model (Lasso): A trained Lasso regression model.\n",
    "    X (DataFrame): Features data.\n",
    "    y_train (Series): Target variable.\n",
    "    feature_names (list): List of feature names.\n",
    "    n_bootstraps (int): Number of bootstrap samples.\n",
    "    \n",
    "    Returns:\n",
    "    sf_df (DataFrame): DataFrame containing the sorted results.\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.utils import resample\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    # Lists to store bootstrap coefficients, y means, RMSE, and R-squared scores\n",
    "    bootstrap_coefficients_list = []\n",
    "    train_rmse_scores = []\n",
    "    test_rmse_scores = []\n",
    "    train_r2_scores = []\n",
    "    test_r2_scores = []\n",
    "\n",
    "    # Bootstrap loop\n",
    "    for _ in range(n_bootstraps):\n",
    "        X_resampled, y_resampled = resample(X,y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Collect each series of coefficients\n",
    "        bootstrap_coefficients_list.append(pd.Series(model.coef_, index=feature_names))\n",
    "\n",
    "        # Predict and calculate RMSE and R^2 scores for the training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        train_rmse_scores.append(train_rmse)\n",
    "        train_r2_scores.append(train_r2)\n",
    "\n",
    "        # Predict and calculate RMSE and R^2 scores for the testing set\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        test_rmse_scores.append(test_rmse)\n",
    "        test_r2_scores.append(test_r2)\n",
    "        \n",
    "    # Concatenate all series stored in the list into a dataframe\n",
    "    bootstrap_coefficients = pd.concat(bootstrap_coefficients_list, axis=1).transpose()\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the bootstrapped coefficients\n",
    "    coefficient_mean = bootstrap_coefficients.mean()\n",
    "    coefficient_std = bootstrap_coefficients.std()\n",
    "    \n",
    "    # Calculate the 95% confidence intervals for each coefficient\n",
    "    ci_lower = bootstrap_coefficients.quantile(0.025)\n",
    "    ci_upper = bootstrap_coefficients.quantile(0.975)\n",
    "    \n",
    "    # Create a DataFrame to summarize the results\n",
    "    coefficient_summary = pd.DataFrame({\n",
    "        'Mean': coefficient_mean,\n",
    "        'StdDev': coefficient_std,\n",
    "        'CI Lower': ci_lower,\n",
    "        'CI Upper': ci_upper\n",
    "    })\n",
    "    \n",
    "    # Select significant features based on the absolute value of the coefficient mean and distribution\n",
    "    selected_features = coefficient_summary.index.tolist()\n",
    "\n",
    "    # Create a DataFrame from the bootstrap results and sort it by the mean coefficient\n",
    "    sf_df = coefficient_summary.loc[selected_features].sort_values(by='Mean', ascending=False)\n",
    "\n",
    "    # Calculate and store the average of the target variable\n",
    "    avg_y = np.mean(y)\n",
    "\n",
    "    # Calculate and store the mean RMSE and R^2 scores for both training and testing sets\n",
    "    avg_train_rmse = np.mean(train_rmse_scores)\n",
    "    avg_test_rmse = np.mean(test_rmse_scores)\n",
    "    avg_train_r2 = np.mean(train_r2_scores)\n",
    "    avg_test_r2 = np.mean(test_r2_scores)\n",
    "\n",
    "    # Print the evaluation metrics and the sorted coefficients\n",
    "    print(f'Average Y: {avg_y}')\n",
    "    print(f'Average Training RMSE: {avg_train_rmse}, Average Testing RMSE: {avg_test_rmse}')\n",
    "    print(f'Average Training R^2: {avg_train_r2}, Average Testing R^2: {avg_test_r2}')\n",
    "    print(f'Sorted Coefficients: \\n{sf_df}')\n",
    "    \n",
    "    return sf_df\n",
    "\n",
    "# Create a function to generate an error bar chart containing regression coefficients with 95% confidence intervals to the target feature\n",
    "def gen_ebar(bootstrap_results):\n",
    "    \"\"\"\n",
    "    Function to generate and display an error bar chart containing feature regression coefficients with 95% confidence intervals to the target feature.\n",
    "\n",
    "    Parameters:\n",
    "    bootstrap_results (pd.DataFrame): DataFrame containing the mean, standard deviation, and lower/upper confidence intervals of the regression \n",
    "    coefficients to the target feature.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Set the width proportionally with the length of coefficients\n",
    "    width = len(bootstrap_results)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(width, 6))\n",
    "    plt.errorbar(\n",
    "        bootstrap_results.index,\n",
    "        bootstrap_results['Mean'],\n",
    "        yerr=[\n",
    "            bootstrap_results['Mean'] - bootstrap_results['CI Lower'],\n",
    "            bootstrap_results['CI Upper'] - bootstrap_results['Mean']\n",
    "        ],\n",
    "        fmt='o',\n",
    "        capsize=5\n",
    "    )\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "    plt.title('Regression Coefficients with 95% Confidence Intervals')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "# Create a function to calculate the effects of the selected features on the target\n",
    "def effect_on_target(bootstrap_results, df, is_percentage=True):\n",
    "    \"\"\"\n",
    "    Calculate the effect on the target for each feature after bootstrapping occurs and print the results.\n",
    "\n",
    "    Parameters:\n",
    "    target (string): Name of target variable.\n",
    "    bootstrap_results (pd.DataFrame): DataFrame containing feature statistics with columns ['Mean', 'StdDev', 'CI Lower', 'CI Upper'].\n",
    "    df (pd.DataFrame): Original DataFrame containing features.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Import the needed libraries\n",
    "    import numpy as np\n",
    "    \n",
    "    for index, row in bootstrap_results.iterrows():\n",
    "        ci_lower = row['CI Lower']\n",
    "        ci_upper = row['CI Upper']\n",
    "\n",
    "        # Exclude features whose confidence intervals include 0\n",
    "        if ci_lower <= 0 <= ci_upper:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        feature_name = index\n",
    "        regression_coefficient = row['Mean']\n",
    "\n",
    "        # Calculate the increment as 2% of the mean value of the feature\n",
    "        feature_mean = np.mean(df[feature_name])\n",
    "        increment = feature_mean * 0.02\n",
    "\n",
    "        # Get the standard deviation of the original (non-scaled) feature\n",
    "        feature_std = np.std(df[feature_name])\n",
    "\n",
    "        # Calculate the scaled feature increment\n",
    "        delta_x_scaled = increment / feature_std\n",
    "\n",
    "        # Calculate the effect on the target variable\n",
    "        delta_y = regression_coefficient * delta_x_scaled\n",
    "\n",
    "        # Print the effect statement\n",
    "        if is_percentage:\n",
    "            print(f'A {increment:.3f} unit increase in {feature_name} results in an approximate {delta_y:.4f} (or {delta_y * 100:.2f} percentage points) change in the target variable.')\n",
    "        else:\n",
    "            print(f\"A {increment:.3f} unit increase in {feature_name} results in an approximate {delta_y:.3f} change in the target variable.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
